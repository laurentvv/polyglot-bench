# Contributing to Multi-Language Performance Benchmark Tool

Thank you for your interest in contributing to the Multi-Language Performance Benchmark Tool! This document provides guidelines and information to help you contribute effectively.

## üìã Table of Contents

- [Code of Conduct](#code-of-conduct)
- [Getting Started](#getting-started)
- [How to Contribute](#how-to-contribute)
- [Development Process](#development-process)
- [Code Style](#code-style)
- [Testing](#testing)
- [Documentation](#documentation)
- [Pull Request Process](#pull-request-process)
- [Community](#community)

## ü§ù Code of Conduct

This project follows a code of conduct that promotes respectful and inclusive communication. By participating, you agree to maintain a welcoming environment for all contributors.

## üöÄ Getting Started

1. Fork the repository
2. Clone your fork: `git clone https://github.com/yourusername/multi-language-benchmark.git`
3. Create a branch for your contribution: `git checkout -b feature/your-feature-name`
4. Set up the development environment following the instructions in [README.md](README.md)

## üí° How to Contribute

There are many ways to contribute to this project:

### Add New Benchmarks
- Implement new test cases in existing benchmark categories
- Create entirely new benchmark categories
- Add language-specific optimizations

### Support New Languages
- Extend the tool to support additional programming languages
- Implement language runners for new languages
- Add environment validation for new languages

### Improve Existing Tests
- Optimize current benchmark implementations
- Add new test parameters or configurations
- Improve accuracy and reliability of measurements

### Enhance Reporting
- Add new visualization types
- Create additional report formats
- Improve existing report layouts and information

### Bug Fixes
- Fix issues with existing implementations
- Improve error handling and edge cases
- Enhance cross-platform compatibility

### Documentation
- Improve existing documentation
- Add tutorials and examples
- Translate documentation to other languages

## üîß Development Process

### Project Structure
The project follows a modular architecture:
- `src/orchestrator/`: Core benchmark coordination logic
- `src/utils/`: Utility functions and helpers
- `tests/`: Individual benchmark implementations
- `scripts/`: Development and maintenance scripts

### Adding New Benchmarks
1. Create a new directory in the appropriate category under `tests/`
2. Implement the test in all supported languages:
   - Python: `test_name.py`
   - Rust: `test_name.rs`
   - Go: `test_name.go`
   - TypeScript: `test_name.ts`
3. Create an `input.json` file with test parameters
4. Follow the existing test structure and conventions

### Adding New Languages
1. Create a new language runner in `src/orchestrator/runners.py`
2. Add language configuration to `bench.config.json`
3. Implement environment validation in `src/utils/validation.py`
4. Create sample test implementations

## üé® Code Style

### Python
Follow [PEP 8](https://www.python.org/dev/peps/pep-0008/) style guidelines:
- Use 4 spaces for indentation
- Limit lines to 88 characters (Black formatter standard)
- Use descriptive variable and function names
- Include docstrings for all public functions and classes

### Rust
Follow [Rust Style Guide](https://github.com/rust-dev-tools/fmt-rfcs/blob/master/guide/guide.md):
- Use `rustfmt` for formatting
- Follow Rust naming conventions
- Include documentation comments for public APIs

### Go
Follow [Effective Go](https://golang.org/doc/effective_go.html):
- Use `gofmt` for formatting
- Follow Go naming conventions
- Include package comments

### TypeScript
Follow [TypeScript Coding Guidelines](https://github.com/Microsoft/TypeScript/wiki/Coding-guidelines):
- Use `prettier` for formatting
- Include JSDoc comments for public APIs
- Use strict typing

## üß™ Testing

### Test Structure
Each benchmark test should:
1. Accept input parameters from a JSON file
2. Produce standardized output in JSON format
3. Measure execution time accurately
4. Handle errors gracefully
5. Clean up resources properly

### Validation Tests
Before submitting your contribution:
1. Run the validation script: `python bench_orchestrator.py validate`
2. Test your implementation with the orchestrator
3. Verify all languages compile and run correctly
4. Check that output format matches the specification

### Performance Testing
Ensure your contributions don't negatively impact performance:
1. Profile your code for bottlenecks
2. Test with various input sizes
3. Verify memory usage is reasonable

## üìö Documentation

### Code Documentation
- Include docstrings/comments for all public APIs
- Document function parameters and return values
- Explain complex algorithms or logic
- Include usage examples where appropriate

### README Updates
- Update the main README.md with new features
- Add new sections for significant additions
- Keep installation instructions current
- Include examples for new functionality

### Inline Comments
- Comment complex or non-obvious code
- Explain why certain approaches were chosen
- Reference relevant resources or papers
- Keep comments up to date with code changes

## üì• Pull Request Process

### Before Submitting
1. Ensure your code follows the style guidelines
2. Run all tests and verify they pass
3. Update documentation as needed
4. Add yourself to the contributors list (if desired)

### Pull Request Guidelines
1. **Title**: Use a clear, descriptive title
2. **Description**: Explain what your PR does and why
3. **Related Issues**: Reference any related issues
4. **Testing**: Describe how you tested your changes
5. **Screenshots**: Include screenshots for UI changes

### Review Process
1. At least one maintainer will review your PR
2. Address any feedback or requested changes
3. Once approved, your PR will be merged
4. Thank you for your contribution!

## üåê Community

### Communication Channels
- GitHub Issues: For bug reports and feature requests
- GitHub Discussions: For general discussion and questions
- Email: For private inquiries (see maintainers in README)

### Getting Help
If you need help with your contribution:
1. Check existing documentation and examples
2. Search issues and discussions for similar topics
3. Ask questions in GitHub Discussions
4. Reach out to maintainers directly if needed

## üôè Thank You

Thank you for considering contributing to the Multi-Language Performance Benchmark Tool! Your contributions help make this project better for everyone.